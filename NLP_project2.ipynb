{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0        8\n",
      "1       10\n",
      "2        8\n",
      "3       11\n",
      "4        9\n",
      "        ..\n",
      "1195     9\n",
      "1196     8\n",
      "1197     8\n",
      "1198     8\n",
      "1199    11\n",
      "Name: 分数, Length: 1200, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "DATASET_DIR = './'\n",
    "df = pd.read_excel( '作文打分语料（英语）.xlsx', sheet_name= 0)\n",
    "y = df['分数']\n",
    "X = df\n",
    "\n",
    "print(type(X))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>作文</th>\n",
       "      <th>分数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I believe the computer d...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I have heard the concern o...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@CAPS1 you know we all like computers and we a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I Believe that computers have a positive effec...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear newstimes, I really think that computers ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   编号                                                 作文  分数\n",
       "0   1  Dear Local Newspaper, I believe the computer d...   8\n",
       "1   2  Dear @CAPS1 @CAPS2, I have heard the concern o...  10\n",
       "2   3  @CAPS1 you know we all like computers and we a...   8\n",
       "3   4  I Believe that computers have a positive effec...  11\n",
       "4   5  Dear newstimes, I really think that computers ...   9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features, maxlen):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((maxlen, num_features),dtype=\"float32\")\n",
    "    \n",
    "    counter = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            featureVec[counter] = model[word]\n",
    "        counter = counter + 1\n",
    "        if counter >= maxlen:\n",
    "            break\n",
    "    \n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    maxlen = 200\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays), maxlen, num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features, maxlen)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FengYJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对原始数据进行一些统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的句子词数是 785\n",
      "第一条句子： ['dear', 'local', 'newspaper', 'i', 'believe', 'the', 'computer', 'does', 'have', 'positive', 'perks', 'but', 'it', 'also', 'has', 'negative', 'perks']\n",
      "最长的文章词数是 427\n",
      "平均的文章词数是 185.85583333333332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "essays = X['作文']\n",
    "\n",
    "sentences = []\n",
    "for essay in essays:\n",
    "    essay = essay.split()\n",
    "    #print(len(essay))\n",
    "    sentences.append(len(essay))\n",
    "    #break\n",
    "#print(sentences)\n",
    "print(\"最长的句子词数是\",max(sentences))\n",
    "\n",
    "sentences = []\n",
    "for essay in essays:\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
    "print(\"第一条句子：\",sentences[0])\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay in essays:\n",
    "    #essay_to_wordlist(essay_v, remove_stopwords=True)\n",
    "    #print(essay_to_wordlist(essay, remove_stopwords=True))\n",
    "    clean_train_essays.append(len(essay_to_wordlist(essay, remove_stopwords=True)))\n",
    "    #break\n",
    "print(\"最长的文章词数是\",max(clean_train_essays))\n",
    "print(\"平均的文章词数是\",sum(clean_train_essays)/len(clean_train_essays))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型一：两层LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Conv1D, GlobalMaxPooling1D, Masking\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    #是否使用变长序列\n",
    "    #model.add(Masking(mask_value= 0,input_shape=(200, 300,)))\n",
    "    \n",
    "    model.add(LSTM(250, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300], return_sequences=True))\n",
    "    model.add(LSTM(128, recurrent_dropout=0.4))\n",
    "    \n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型二：加入CNN卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements LSTM with Mean over Time layer.\n",
    "\"\"\"\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, Lambda\n",
    "from keras.models import Sequential\n",
    "import keras.regularizers\n",
    "import keras.backend as K\n",
    "def get_model2():\n",
    "    \"\"\"\n",
    "    Returns compiled model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=50, kernel_size=5, padding='same', input_shape=[200, 300]))\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "    model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu', activity_regularizer=keras.regularizers.l2(0.0)))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型三：一层LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model3():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300]))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Lambda(lambda x: K.mean(x, axis=1, keepdims=True)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型四：双向LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model4():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=64, dropout=0.4, return_sequences=True),input_shape=[200, 300]))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300]))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "['dear', 'caps', 'caps', 'heard', 'concern', 'many', 'scientists', 'computers']\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 200, 300)\n",
      "(960, 200, 300)\n",
      "(960,)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 200, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 188,961\n",
      "Trainable params: 188,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 5s 5ms/step - loss: 26.2021 - mae: 4.2339\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 8.4183 - mae: 2.2986\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 8.2720 - mae: 2.2777\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 7.1242 - mae: 2.0886\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 7.4028 - mae: 2.1786\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 6.6112 - mae: 2.0392\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 7.3527 - mae: 2.1806\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 4s 5ms/step - loss: 6.7514 - mae: 2.0699\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 7.2343 - mae: 2.1275\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 4s 4ms/step - loss: 7.6564 - mae: 2.2252\n",
      "\n",
      "Spearman’s ρ Score: 0.7378893224034999\n",
      "Pearson r Score: 0.7130340615067892\n",
      "RMSE Score: 1.2616127245183708\n",
      "Cohen’s κ Score: 0.5896794726074108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "#     print(X_train)\n",
    "    train_essays = X_train['作文']\n",
    "    test_essays = X_test['作文']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "    \n",
    "    print(sentences[0])\n",
    "    \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "\n",
    "    #print(train_essays)\n",
    "    #print(clean_train_essays[0])\n",
    "    #print(len(clean_train_essays[0]))\n",
    "    \n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    #print(trainDataVecs[0])\n",
    "    #print(len(trainDataVecs))\n",
    "    print(trainDataVecs.shape)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], trainDataVecs.shape[1], trainDataVecs.shape[2]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], trainDataVecs.shape[1], testDataVecs.shape[2]))\n",
    "    print(trainDataVecs.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    #****************************************************************************8\n",
    "    lstm_model = get_model4()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=10)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 5 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    kappa = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['true'] = y_test.values\n",
    "    df2['pred'] = y_pred\n",
    "    spear = df2.corr('spearman')['pred'][0]\n",
    "    pearson = df2.corr('pearson')['pred'][0]\n",
    "    RMSE = sqrt(mean_squared_error(y_test.values, y_pred))\n",
    "    print()\n",
    "    print(\"Spearman’s ρ Score:\", spear)\n",
    "    print(\"Pearson r Score:\", pearson)\n",
    "    print(\"RMSE Score:\", RMSE)\n",
    "    print(\"Cohen’s κ Score:\", kappa)\n",
    "   # print(y_test.values)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    results.append(kappa)\n",
    "    \n",
    "    count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#embedding处理判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "第一条句子： ['dear', 'local', 'newspaper', 'believe', 'computer', 'positive', 'perks', 'also', 'negative', 'perks']\n",
      "Training Word2Vec Model...\n",
      "['dear', 'local', 'newspaper', 'believe', 'computer', 'positive', 'perks', 'also', 'negative', 'perks', 'reason', 'one', 'loose', 'interaction', 'people', 'reason', 'two', 'forget', 'excersize', 'reason', 'three', 'health', 'jeopardy', 'firstly', 'loose', 'interaction', 'family', 'friends', 'would', 'spend', 'much', 'time', 'computers', 'ruins', 'marriage', 'married', 'kids', 'risk', 'relationship', 'saying', 'stop', 'using', 'computers', 'use', 'saying', 'make', 'life', 'computer', 'good', 'secondly', 'need', 'excersize', 'stastics', 'show', 'percent', 'people', 'live', 'computers', 'gain', 'weight', 'faster', 'would', 'know', 'num', 'yr', 'old', 'gained', 'little', 'weight', 'computers', 'people', 'computer', 'tend', 'eat', 'caps', 'well', 'known', 'facts', 'proven', 'since', 'walk', 'computer', 'hand', 'stop', 'moving', 'meaning', 'stop', 'excersizing', 'limit', 'time', 'believe', 'computers', 'benfit', 'lastly', 'health', 'much', 'stake', 'computer', 'num', 'hours', 'time', 'eye', 'site', 'pay', 'eyes', 'stink', 'computer', 'half', 'hour', 'head', 'hurt', 'computer', 'long', 'yes', 'computer', 'fun', 'gives', 'things', 'first', 'one', 'admit', 'also', 'hurts', 'heath', 'computer', 'long', 'risk', 'carpal', 'tunal', 'syndrome', 'painful', 'computers', 'much', 'fun', 'make', 'life', 'easier', 'disadvantages', 'reason', 'one', 'loose', 'interaction', 'people', 'reason', 'two', 'stop', 'excersizing', 'reason', 'three', 'health', 'risk', 'limit', 'time', 'everythig', 'alright']\n",
      "155\n",
      "一篇文章的shape： (1, 200, 300)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "不在model中\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "#     print(X_train)\n",
    "    train_essays = X_train['作文']\n",
    "    test_essays = X_test['作文']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "    \n",
    "    print(\"第一条句子：\",sentences[0])\n",
    "    \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "        break\n",
    "    #print(train_essays)\n",
    "    print(clean_train_essays[0])\n",
    "    print(len(clean_train_essays[0]))\n",
    "    \n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    #print(trainDataVecs[0])\n",
    "    #print(len(trainDataVecs))\n",
    "    print(\"一篇文章的shape：\",trainDataVecs.shape)\n",
    "    print(trainDataVecs[0,len(clean_train_essays[0])-1])\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    if clean_train_essays[0][-1] in index2word_set:\n",
    "        print(model[clean_train_essays[0][-1]])\n",
    "    else:\n",
    "        print(\"不在model中\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
