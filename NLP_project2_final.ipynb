{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **英文作文打分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据量： 1200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "DATASET_DIR = './'\n",
    "df = pd.read_excel( '作文打分语料（英语）.xlsx', sheet_name= 0)\n",
    "print(\"数据量：\",len(df))\n",
    "#X 是输入； y 是输出\n",
    "X = df\n",
    "y = df['分数']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>作文</th>\n",
       "      <th>分数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I believe the computer d...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I have heard the concern o...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@CAPS1 you know we all like computers and we a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I Believe that computers have a positive effec...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear newstimes, I really think that computers ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   编号                                                 作文  分数\n",
       "0   1  Dear Local Newspaper, I believe the computer d...   8\n",
       "1   2  Dear @CAPS1 @CAPS2, I have heard the concern o...  10\n",
       "2   3  @CAPS1 you know we all like computers and we a...   8\n",
       "3   4  I Believe that computers have a positive effec...  11\n",
       "4   5  Dear newstimes, I really think that computers ...   9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始excel文件中包含训练语料和测试预料两部分。<br />\n",
    "由于测试预料中没有评分结果，所以我们只使用训练语料采用5折交叉验证的方法来进行评估。<br />\n",
    "通过上面代码，我们可以看到一共有1200条数据可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步 定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#功能：对输入的句子进行处理，移除非字母符号，并根据remove_stopwords来判断是否移除stopwords，返回处理后的句子。\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "#功能：对输入的一篇文章进行tokenize处理，同时调用上面的函数对每一个句子进行处理。返回处理后的多条句子。\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "#根据训练好的word2vec模型，获取每个词的词向量\n",
    "def makeFeatureVec(words, model, num_features, maxlen):\n",
    "    featureVec = np.zeros((maxlen, num_features),dtype=\"float32\")\n",
    "    \n",
    "    counter = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            featureVec[counter] = model[word]\n",
    "        counter = counter + 1\n",
    "        if counter >= maxlen:\n",
    "            break\n",
    "    \n",
    "    return featureVec\n",
    "\n",
    "#输入文章和word2vec模型，以及编码的维度，返回一篇文章的所有词向量\n",
    "def getAllFeatureVecs(essays, model, num_features):\n",
    "    #参数，每句话的最大长度\n",
    "    maxlen = 200\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays), maxlen, num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features, maxlen)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FengYJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步 对原始数据进行简单分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的句子词数是 785\n",
      "第一条句子经处理后的结果： ['dear', 'local', 'newspaper', 'i', 'believe', 'the', 'computer', 'does', 'have', 'positive', 'perks', 'but', 'it', 'also', 'has', 'negative', 'perks']\n",
      "第一批篇文章经处理后的结果： ['dear', 'local', 'newspaper', 'believe', 'computer', 'positive', 'perks', 'also', 'negative', 'perks', 'reason', 'one', 'loose', 'interaction', 'people', 'reason', 'two', 'forget', 'excersize', 'reason', 'three', 'health', 'jeopardy', 'firstly', 'loose', 'interaction', 'family', 'friends', 'would', 'spend', 'much', 'time', 'computers', 'ruins', 'marriage', 'married', 'kids', 'risk', 'relationship', 'saying', 'stop', 'using', 'computers', 'use', 'saying', 'make', 'life', 'computer', 'good', 'secondly', 'need', 'excersize', 'stastics', 'show', 'percent', 'people', 'live', 'computers', 'gain', 'weight', 'faster', 'would', 'know', 'num', 'yr', 'old', 'gained', 'little', 'weight', 'computers', 'people', 'computer', 'tend', 'eat', 'caps', 'well', 'known', 'facts', 'proven', 'since', 'walk', 'computer', 'hand', 'stop', 'moving', 'meaning', 'stop', 'excersizing', 'limit', 'time', 'believe', 'computers', 'benfit', 'lastly', 'health', 'much', 'stake', 'computer', 'num', 'hours', 'time', 'eye', 'site', 'pay', 'eyes', 'stink', 'computer', 'half', 'hour', 'head', 'hurt', 'computer', 'long', 'yes', 'computer', 'fun', 'gives', 'things', 'first', 'one', 'admit', 'also', 'hurts', 'heath', 'computer', 'long', 'risk', 'carpal', 'tunal', 'syndrome', 'painful', 'computers', 'much', 'fun', 'make', 'life', 'easier', 'disadvantages', 'reason', 'one', 'loose', 'interaction', 'people', 'reason', 'two', 'stop', 'excersizing', 'reason', 'three', 'health', 'risk', 'limit', 'time', 'everythig', 'alright']\n",
      "最长的文章词数是 427\n",
      "平均的文章词数是 185.85583333333332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "essays = X['作文']\n",
    "\n",
    "sentences = []\n",
    "for essay in essays:\n",
    "    essay = essay.split()\n",
    "    #print(len(essay))\n",
    "    sentences.append(len(essay))\n",
    "    #break\n",
    "#print(sentences)\n",
    "print(\"最长的句子词数是\",max(sentences))\n",
    "\n",
    "sentences = []\n",
    "for essay in essays:\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords = False)\n",
    "print(\"第一条句子经处理后的结果：\",sentences[0])\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay in essays:\n",
    "    #essay_to_wordlist(essay_v, remove_stopwords=True)\n",
    "    #print(essay_to_wordlist(essay, remove_stopwords=True))\n",
    "    clean_train_essays.append(len(essay_to_wordlist(essay, remove_stopwords=True)))\n",
    "    #break\n",
    "for essay in essays:\n",
    "    print(\"第一批篇文章经处理后的结果：\",essay_to_wordlist(essay, remove_stopwords=True))\n",
    "    break\n",
    "print(\"最长的文章词数是\",max(clean_train_essays))\n",
    "print(\"平均的文章词数是\",sum(clean_train_essays)/len(clean_train_essays))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出，1200篇文章经处理后，每篇平均的单词数为185，这里可以方便我们选择实验参数maxlen（每句话的最大长度）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步 定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们共使用了四种模型，从模型的复杂程度进行了排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型一：一层LSTM<br />\n",
    "LSTM的一大优势就是其能够处理变长序列,我们可以通过在LSTM层前加一个Masking层来实现此功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model1():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #是否使用变长序列\n",
    "    #model.add(Masking(mask_value= 0,input_shape=(200, 300,)))\n",
    "    \n",
    "    model.add(LSTM(256, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300]))\n",
    "    model.add(Dropout(0.4))\n",
    "    #model.add(Lambda(lambda x: K.mean(x, axis=1, keepdims=True)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型二：两层LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Conv1D, GlobalMaxPooling1D, Masking\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model2():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    #是否使用变长序列\n",
    "#     model.add(Masking(mask_value= 0,input_shape=(200, 300,)))\n",
    "#     model.add(LSTM(250, dropout=0.4, recurrent_dropout=0.4, return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(250, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300], return_sequences=True))\n",
    "    model.add(LSTM(128, recurrent_dropout=0.4))\n",
    "    \n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型三：加入CNN卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, Lambda\n",
    "from keras.models import Sequential\n",
    "import keras.regularizers\n",
    "import keras.backend as K\n",
    "def get_model3():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=50, kernel_size=5, padding='same', input_shape=[200, 300]))\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "    model.add(Lambda(lambda x: K.mean(x, axis=1)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu', activity_regularizer=keras.regularizers.l2(0.0)))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型四：双向LSTM + 池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model4():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(units=64, dropout=0.4, return_sequences=True),input_shape=[200, 300]))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    #model.add(LSTM(64, dropout=0.4, recurrent_dropout=0.4, input_shape=[200, 300]))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步 模型训练和结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过选择上述不同的模型来得到不同的结果。\n",
    "经调试，在此数据集上效果最好的是model3。（LSTM+CNN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 200, 50)           75050     \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 200, 300)          421200    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 496,551\n",
      "Trainable params: 496,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 19s 20ms/step - loss: 21.8025\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.8039\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.3845\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.2177\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.9907\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 18s 18ms/step - loss: 1.8826\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.7462\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.5550\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4889\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4557\n",
      "\n",
      "Spearman’s ρ Score: 0.6287116571260717\n",
      "Pearson r Score: 0.7198369499004663\n",
      "RMSE Score: 1.0801234497346435\n",
      "Cohen’s κ Score: 0.7037063448202092\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 200, 50)           75050     \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 200, 300)          421200    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 496,551\n",
      "Trainable params: 496,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 19s 20ms/step - loss: 25.5881\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 18s 18ms/step - loss: 3.2382\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.4701\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.2684\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.1339\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 19s 20ms/step - loss: 1.9449\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.8638\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.6333\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.5676\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.5402\n",
      "\n",
      "Spearman’s ρ Score: 0.620671614284984\n",
      "Pearson r Score: 0.6868707652033452\n",
      "RMSE Score: 1.0878112581387147\n",
      "Cohen’s κ Score: 0.6631166703298899\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 200, 50)           75050     \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 200, 300)          421200    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 496,551\n",
      "Trainable params: 496,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 19s 19ms/step - loss: 24.9220\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.9800\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 19s 19ms/step - loss: 2.5692\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.3640\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.1751\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.9247\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.7454\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.6851\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4544\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4766\n",
      "\n",
      "Spearman’s ρ Score: 0.6362543086348958\n",
      "Pearson r Score: 0.7240151445154587\n",
      "RMSE Score: 1.0801234497346435\n",
      "Cohen’s κ Score: 0.6998356764652182\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 200, 50)           75050     \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 200, 300)          421200    \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 496,551\n",
      "Trainable params: 496,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 23.2408\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 21s 21ms/step - loss: 2.8926\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 19s 19ms/step - loss: 2.5028\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.3550\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.0011\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.8779\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.6840\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.7179\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.6203\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4562\n",
      "\n",
      "Spearman’s ρ Score: 0.7264601206663767\n",
      "Pearson r Score: 0.7242146956473988\n",
      "RMSE Score: 1.0858944086174616\n",
      "Cohen’s κ Score: 0.6650834246742174\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\other-software\\Anaconda\\envs\\nlp37\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 200, 50)           75050     \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 200, 300)          421200    \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 496,551\n",
      "Trainable params: 496,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "960/960 [==============================] - 19s 19ms/step - loss: 22.8316\n",
      "Epoch 2/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 3.0170\n",
      "Epoch 3/10\n",
      "960/960 [==============================] - 18s 18ms/step - loss: 2.4390\n",
      "Epoch 4/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.4071\n",
      "Epoch 5/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 2.1073\n",
      "Epoch 6/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.7745\n",
      "Epoch 7/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.7654\n",
      "Epoch 8/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.6355\n",
      "Epoch 9/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.5692\n",
      "Epoch 10/10\n",
      "960/960 [==============================] - 18s 19ms/step - loss: 1.4109\n",
      "\n",
      "Spearman’s ρ Score: 0.6454436669439431\n",
      "Pearson r Score: 0.7034779671944342\n",
      "RMSE Score: 1.1086778913041726\n",
      "Cohen’s κ Score: 0.6514082383450938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#5折交叉验证\n",
    "cv = KFold(n_splits = 5, shuffle = True)\n",
    "results_spear = []\n",
    "results_perason = []\n",
    "results_RMSE = []\n",
    "results_kappa = []\n",
    "\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "#     print(X_train)\n",
    "    train_essays = X_train['作文']\n",
    "    test_essays = X_test['作文']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "    \n",
    "    #print(sentences[0])\n",
    "    \n",
    "    # 设置 word2vec 参数\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    # 生成训练和测试的词向量集\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "\n",
    "    #print(train_essays)\n",
    "    #print(clean_train_essays[0])\n",
    "    #print(len(clean_train_essays[0]))\n",
    "    \n",
    "    trainDataVecs = getAllFeatureVecs(clean_train_essays, model, num_features)\n",
    "    #print(trainDataVecs[0])\n",
    "    #print(len(trainDataVecs))\n",
    "    #print(trainDataVecs.shape)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAllFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    \n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], trainDataVecs.shape[1], trainDataVecs.shape[2]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], trainDataVecs.shape[1], testDataVecs.shape[2]))\n",
    "    #print(trainDataVecs.shape)\n",
    "    #print(y_train.shape)\n",
    "    \n",
    "    #****************************************************************************\n",
    "    #选择不同的模型！！\n",
    "    lstm_model = get_model3()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=10)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # 模型评估\n",
    "    kappa = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    df2 = pd.DataFrame()\n",
    "    df2['true'] = y_test.values\n",
    "    df2['pred'] = y_pred\n",
    "    spear = df2.corr('spearman')['pred'][0]\n",
    "    pearson = df2.corr('pearson')['pred'][0]\n",
    "    RMSE = sqrt(mean_squared_error(y_test.values, y_pred))\n",
    "    print()\n",
    "    print(\"Spearman’s ρ Score:\", spear)\n",
    "    print(\"Pearson r Score:\", pearson)\n",
    "    print(\"RMSE Score:\", RMSE)\n",
    "    print(\"Cohen’s κ Score:\", kappa)\n",
    "   # print(y_test.values)\n",
    "    #print(y_pred)\n",
    "    \n",
    "    results_spear.append(spear)\n",
    "    results_perason.append(pearson)\n",
    "    results_RMSE.append(RMSE)\n",
    "    results_kappa.append(kappa)\n",
    "    \n",
    "    count += 1\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Spearman’s ρ score after a 5-fold cross validation:  0.6515\n",
      "Average Pearson r score after a 5-fold cross validation:  0.7117\n",
      "Average RMSE score after a 5-fold cross validation:  1.0885\n",
      "Average Kappa score after a 5-fold cross validation:  0.6766\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Spearman’s ρ score after a 5-fold cross validation: \",np.around(np.array(results_spear).mean(),decimals=4))\n",
    "print(\"Average Pearson r score after a 5-fold cross validation: \",np.around(np.array(results_perason).mean(),decimals=4))\n",
    "print(\"Average RMSE score after a 5-fold cross validation: \",np.around(np.array(results_RMSE).mean(),decimals=4))\n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results_kappa).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
